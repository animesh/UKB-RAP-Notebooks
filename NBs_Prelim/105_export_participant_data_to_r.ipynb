{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export participant data to R\n",
    "\n",
    "> Scope: This notebook shows how to retrieve and export phenotypic data in R\n",
    "\n",
    "Run info: \n",
    "- runtime: 10min \n",
    "- recommended instance: mem1_ssd1_v2_x8\n",
    "- estimated cost: <Â£0.15\n",
    "\n",
    "This notebook depends on:\n",
    "* **A Spark instance**\n",
    "\n",
    "This notebook explains how to retrieve and save phenotypic data for further analyses, such as genome-wide association studies or epidemiological studies. \n",
    "We will use a `reticulate` R package to connect to Python and call the `dxdata.connect` function, which connects to the Spark database. \n",
    "Next, we will convert Python (Spark data frame) object to an R object (tibble) and export data to a tabular text file. This file can be used as an input to external tools, such as PLINK or regenie.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "Function `p_load` from `pacman` loads packages into R.\n",
    "If the given package is missing `p_load` will automatically install it - this can take a considerable amount of time for a package that needs C or FORTRAN code compilation.\n",
    "The following packages are needed to run this notebook:\n",
    "\n",
    "- `reticulate` - R-Python interface, required to use `dxdata` package that connects to Spark database and allows retrieval of phenotypic data \n",
    "- `dplyr` - tabular data manipulation in R, require to pre-process, encode and filter phenotypic data\n",
    "- `parallel` - parallel computation in R\n",
    "- `arrow` - input/output library for Apache binary files\n",
    "- `skimr` - provide summary statistics about variables in data frames, tibble objects, data tables and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "message('Installing packages...')\n",
    "if(!require(pacman)) install.packages(\"pacman\")\n",
    "pacman::p_load(reticulate, dplyr, parallel, readr, skimr, arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dxdata package and initialize Spark (dxdata) engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dxdata <- import(\"dxdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the dataset\n",
    "\n",
    "Next, we can set a `DATASET_ID` variable, which takes a value: `[projectID]:[dataset ID]`\n",
    "We use it to define the `dataset` with `dxdata.load_dataset` function.\n",
    "\n",
    "**projectID** and **dataset ID** values are unique to your project.\n",
    "Notebook example **101** explains how to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "project <- system(\"dx env | grep project- | awk -F '\\t' '{print $2}'\", intern = TRUE)\n",
    "record <- system(\"dx describe *dataset | grep  record- | awk -F ' ' '{print $2}' | head -n 1\" , intern = TRUE)\n",
    "DATASET_ID <- paste0(project, \":\", record)\n",
    "dataset <- dxdata$load_dataset(id=DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Select the `participant` table\n",
    "\n",
    "The following code selects the `participant` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pheno <- dataset$entities_by_name[['participant']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select fields from `participant` table\n",
    "\n",
    "\n",
    "We can define which field we are interested in using the `find_field` function.\n",
    "\n",
    "There are three main ways to identify the field of interest:\n",
    "\n",
    "- With `name` argument: we give the field ID. We can construct filed ID used by `dxdata` package from the field ID defined by UKB Showcase. The numeric showcase ID is translated to Spark DB column name by adding the letter `p` at the beginning: e.g. *Standing height* showcase id is `50`, so Spark ID would be `p50`. Usually, fields have multiple instances. In such case, we add `_i` suffix followed by instance number, e.g. *Standing height | Instance 0* will be `p50_i0`\n",
    "- With `title` argument: we define the field by the full title, followed by ` | Instance` suffix, e.g. `Age at recruitment` or `Standing height | Instance 0`\n",
    "- With `title_regex` argument: we define the field by [regular expression](https://docs.python.org/3/howto/regex.html) matching the part of the title. We can use a keyword here, e.g. `.*height.*` will return all columns with the word *height* in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fld = list(\n",
    "    pheno$find_field(name=\"eid\"),\n",
    "    pheno$find_field(title=\"Sex\"),\n",
    "    pheno$find_field(title=\"Age at recruitment\"),\n",
    "    pheno$find_field(title=\"Standing height | Instance 0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Spark engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(hive+pyspark:///)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engine <- dxdata$connect(dialect=\"hive+pyspark\")\n",
    "engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the fields defined in `fld` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df <- pheno$retrieve_fields(engine=engine, fields=fld, coding_values=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the data to a temporary `parquet` file \n",
    "\n",
    "You can learn more about the _parquet_ file format here: [https://parquet.apache.org/](https://parquet.apache.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/latex": [],
      "text/markdown": [],
      "text/plain": [
       "character(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system('hadoop fs -rm -r -f tmpdf.parquet', intern = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df$write$parquet('tmpdf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the temporary _parquet_ file from distributed to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/latex": [],
      "text/markdown": [],
      "text/plain": [
       "character(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(dir.exists('tmpdf.parquet')) unlink(\"tmpdf.parquet\", recursive=TRUE)\n",
    "system('hadoop fs -copyToLocal tmpdf.parquet', intern = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset information R using Apache `arrow` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ds <- arrow::open_dataset('tmpdf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the data from the dataset to R memory\n",
    "\n",
    "Now, the phenotypic data are available as standard `tibble` objects, which can be interacted with using methods from `tidyverse` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tbl <- ds %>% collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "skim(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can inspect your data. For example, you can print 5 first rows using the `head(tbl)` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "readr::write_csv(tbl, 'pheno_height.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data in PLINK phenotype format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pheno_out <- tbl %>% \n",
    "    transmute(\n",
    "        FID=eid, \n",
    "        IID=eid, \n",
    "        Y1=as.double(p50_i0)\n",
    ")\n",
    "\n",
    "pheno_out %>% write_delim(file = 'ukb_phenotypes_height.txt', delim = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "system('dx upload ukb_phenotypes_height.txt --path pheno/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
